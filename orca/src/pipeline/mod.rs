#[cfg(feature = "unstable")]
pub mod mapreduce;
pub mod simple;
// #[cfg(feature = "unstable")]
pub mod sequential;
use crate::{
    llm::LLMResponse,
    prompt::{context::Context, TemplateEngine},
    record::Record,
};

use anyhow::Result;
use serde_json::Value as JsonValue;
use std::collections::HashMap;

#[async_trait::async_trait]
pub trait Pipeline: Sync + Send {
    /// Executes a given pipeline and produces an LLM response.
    ///
    /// # Returns
    /// - A `Result` containing a `PipelineResult` if successful or an error otherwise.
    async fn execute(&self, target: &str) -> Result<PipelineResult>;

    /// Sets the context for the current pipeline execution using a given data structure.
    ///
    /// # Parameters
    ///
    /// - `context`: A reference to a serializable data structure.
    ///
    /// # Examples
    ///
    /// ```
    /// use orca::pipeline::Pipeline;
    /// use orca::llm::openai::OpenAI;
    /// use orca::prompt::context::Context;
    /// use orca::pipeline::simple::LLMPipeline;
    /// use std::collections::HashMap;
    ///
    /// # #[tokio::main]
    /// # async fn main() {
    /// let client = OpenAI::new();
    /// let mut pipeline = LLMPipeline::new(&client).with_template("my prompt", "Hello, {{name}}!");
    /// let mut data = HashMap::new();
    /// data.insert("name", "LLM");
    /// pipeline.load_context(&Context::new(data).unwrap()).await;
    /// # }
    /// ```
    async fn load_context(&mut self, context: &Context) {
        context.as_object().into_iter().for_each(|(key, value)| {
            self.context().insert(key.to_string(), value.clone());
        });
    }

    /// Loads a given record into the context of the LLM pipeline.
    ///
    /// # Parameters
    /// - `name`: The key/name for the record content in the context.
    /// - `record`: The actual record to load.
    fn load_record(&mut self, name: &str, record: Record) {
        if !self.context().contains_key(name) {
            self.context().insert(name.to_string(), JsonValue::String(record.content.to_string()));
        }
    }

    /// Retrieves the current context of the pipeline.
    ///
    /// # Returns
    /// - A mutable reference to a hashmap containing the current context.
    fn context(&mut self) -> &mut HashMap<String, JsonValue>;

    /// Retrieves the template engine for the current pipeline.
    ///
    /// # Returns
    /// - A mutable reference to the template engine.
    fn template_engine(&mut self) -> &mut TemplateEngine {
        unimplemented!()
    }
}

#[derive(Debug)]
pub struct PipelineResult {
    /// Name of the pipeline which generated the result.
    pub name: String,

    /// LLM response generated by the pipeline.
    llm_response: Option<LLMResponse>,
}

impl PipelineResult {
    /// Constructs a new `PipelineResult` instance with a specified name.
    ///
    /// # Parameters
    /// - `name`: The name or identifier for this result.
    ///
    /// # Returns
    /// - A new `PipelineResult` instance.
    pub fn new(name: String) -> PipelineResult {
        PipelineResult {
            name,
            llm_response: None,
        }
    }

    /// Retrieves the content of the LLM response.
    ///
    /// # Returns
    /// - A string representation of the LLM response content.
    pub fn content(&self) -> String {
        self.llm_response.as_ref().unwrap_or(&LLMResponse::Empty).to_string()
    }

    /// Determines the role associated with the LLM response.
    ///
    /// # Returns
    /// - A string representation of the role in the LLM response.
    pub fn role(&self) -> String {
        self.llm_response.as_ref().unwrap_or(&LLMResponse::Empty).to_role()
    }

    /// Sets the LLM response for the current `PipelineResult`.
    ///
    /// # Parameters
    /// - `llm_response`: The actual LLM response to set.
    ///
    /// # Returns
    /// - The modified `PipelineResult` instance.
    pub fn with_llm_response(mut self, llm_response: LLMResponse) -> Self {
        self.llm_response = Some(llm_response);
        self
    }
}
