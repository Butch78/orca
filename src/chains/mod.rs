pub mod chain;

pub mod mapreduce;
pub mod sequential;
use crate::prompt::clean_prompt;
use crate::{llm::LLMResponse, record::Record};

use anyhow::Result;
use serde::Serialize;
use std::collections::HashMap;

#[async_trait::async_trait]
pub trait Chain {
    /// Executes a given chain and produces an LLM response.
    ///
    /// # Returns
    /// - A `Result` containing a `ChainResult` if successful or an error otherwise.
    async fn execute(&self, target: &str) -> Result<ChainResult>;

    /// Sets the context for the current chain execution using a given data structure.
    ///
    /// # Parameters
    ///
    /// - `context`: A reference to a serializable data structure.
    ///
    /// # Examples
    ///
    /// ```
    /// use orca::chains::Chain;
    /// use orca::llm::openai::OpenAI;
    /// use orca::chains::chain::LLMChain;
    /// use std::collections::HashMap;
    ///
    /// # #[tokio::main]
    /// # async fn main() {
    /// let client = OpenAI::new();
    /// let mut chain = LLMChain::new(&client).with_prompt("my prompt", "Hello, {{name}}!");
    /// let mut data = HashMap::new();
    /// data.insert("name", "LLM");
    /// chain.load_context(&data).await;
    /// # }
    /// ```
    async fn load_context<T>(&mut self, context: &T)
    where
        T: Serialize + Sync,
        Self: Sized,
    {
        let context = serde_json::to_value(context).unwrap_or(serde_json::Value::Null);
        if let serde_json::Value::Object(map) = context {
            map.into_iter().for_each(|(key, value)| {
                let value = value.as_str().unwrap_or("");
                self.context().insert(key, clean_prompt(value, true));
            });
        }
    }

    /// Loads a given record into the context of the LLM chain.
    ///
    /// # Parameters
    /// - `name`: The key/name for the record content in the context.
    /// - `record`: The actual record to load.
    fn load_record(&mut self, name: &str, record: Record) {
        if !self.context().contains_key(name) {
            self.context().insert(
                name.to_string(),
                clean_prompt(record.content.to_string().as_str(), true),
            );
        }
    }

    /// Retrieves the current context of the chain.
    ///
    /// # Returns
    /// - A mutable reference to a hashmap containing the current context.
    fn context(&mut self) -> &mut HashMap<String, String>;
}

#[derive(Debug)]
pub struct ChainResult {
    /// Name of the chain which generated the result.
    pub name: String,

    /// LLM response generated by the chain.
    llm_response: Option<LLMResponse>,
}

impl ChainResult {
    /// Constructs a new `ChainResult` instance with a specified name.
    ///
    /// # Parameters
    /// - `name`: The name or identifier for this result.
    ///
    /// # Returns
    /// - A new `ChainResult` instance.
    pub fn new(name: String) -> ChainResult {
        ChainResult {
            name,
            llm_response: None,
        }
    }

    /// Retrieves the content of the LLM response.
    ///
    /// # Returns
    /// - A string representation of the LLM response content.
    pub fn content(&self) -> String {
        self.llm_response.as_ref().unwrap_or(&LLMResponse::Empty).to_string()
    }

    /// Determines the role associated with the LLM response.
    ///
    /// # Returns
    /// - A string representation of the role in the LLM response.
    pub fn role(&self) -> String {
        self.llm_response.as_ref().unwrap_or(&LLMResponse::Empty).get_role()
    }

    /// Sets the LLM response for the current `ChainResult`.
    ///
    /// # Parameters
    /// - `llm_response`: The actual LLM response to set.
    ///
    /// # Returns
    /// - The modified `ChainResult` instance.
    pub fn with_llm_response(mut self, llm_response: LLMResponse) -> Self {
        self.llm_response = Some(llm_response);
        self
    }
}
